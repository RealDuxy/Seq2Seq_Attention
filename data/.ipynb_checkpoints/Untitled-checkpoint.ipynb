{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'./AutoMaster_TrainSet.csv' does not exist: b'./AutoMaster_TrainSet.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6df2cb653a4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./AutoMaster_TrainSet.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"all\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./AutoMaster_TestSet.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"all\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'./AutoMaster_TrainSet.csv' does not exist: b'./AutoMaster_TrainSet.csv'"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"./AutoMaster_TrainSet.csv\").dropna(how=\"all\")\n",
    "test_df = pd.read_csv(\"./AutoMaster_TestSet.csv\").dropna(how=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train_y:  82943\n",
      "train_y done\n",
      "length of train_x:  82943\n",
      "train_x done\n",
      "length of test_x:  20000\n",
      "test_x done\n"
     ]
    }
   ],
   "source": [
    "def build_train_test_val_list(train_df, test_df):\n",
    "    num_val = int(len(train_df) * 0.1)\n",
    "    num_train = len(train_df) - num_val\n",
    "    num_test = len(test_df)\n",
    "\n",
    "    def df2list(df, num):\n",
    "        # train or val\n",
    "        if df.shape[1] == 6:\n",
    "            # train\n",
    "            if num > 0:\n",
    "                x_df = df.iloc[:num, 1].map(str) + \"/\"\n",
    "                for i in range(2, 5):\n",
    "                    x_df += \"/\" + df.iloc[:num, i].map(str)\n",
    "                y_df = df.iloc[:num,5]\n",
    "            else: # val\n",
    "                x_df = df.iloc[num:, 1].map(str) + \"/\"\n",
    "                for i in range(2, 5):\n",
    "                    x_df += \"/\" + df.iloc[num:, i].map(str)\n",
    "                y_df = df.iloc[num:,5]\n",
    "            return x_df.tolist(), y_df.tolist()\n",
    "        else: # test\n",
    "            x_df = df.iloc[:num, 1].map(str) + \"/\"\n",
    "            for i in range(2, 5):\n",
    "                x_df += \"/\" + df.iloc[:num, i].map(str)\n",
    "            return x_df.tolist()\n",
    "\n",
    "    train_x_list, train_y_list = df2list(train_df, num_train)\n",
    "    test_x_list = df2list(test_df, num_test)\n",
    "    val_x_list, val_y_list = df2list(train_df, -num_train)\n",
    "\n",
    "    return [train_x_list, train_y_list, test_x_list, val_x_list, val_y_list]\n",
    "    \n",
    "    \n",
    "train_x_list, train_y_list, test_x_list = build_train_test_dataset(train_df,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['随时', '联系']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_train_test(train_x_list, train_y_list, test_x_list):\n",
    "    with open(\"train_x.txt\", \"w\") as f:\n",
    "        for sentence_list in train_x_list:\n",
    "            f.write(\" \".join(sentence_list) + \"\\n\")\n",
    "    with open(\"train_y.txt\", \"w\") as f:\n",
    "        for sentence_list in train_y_list:\n",
    "            f.write(\" \".join(sentence_list) + \"\\n\")  \n",
    "    with open(\"test_x.txt\", \"w\") as f:\n",
    "        for sentence_list in test_x_list:\n",
    "            f.write(\" \".join(sentence_list) + \"\\n\")\n",
    "    \n",
    "save_train_test(train_x_list, train_y_list, test_x_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_test(train_x, train_y, test_x):\n",
    "    train_x_list, train_y_list, test_x_list = [], [], []\n",
    "    with open(train_x, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            line = [s for s in line.strip().split(\" \") if s != \"\"]\n",
    "            train_x_list.append(line)\n",
    "    with open(train_y, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            line = [s for s in line.strip().split(\" \") if s != \"\"]\n",
    "            train_y_list.append(line)\n",
    "    with open(test_x, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            line = [s for s in line.strip().split(\" \") if s != \"\"]\n",
    "            test_x_list.append(line)\n",
    "    return train_x_list, train_y_list, test_x_list\n",
    "\n",
    "train_x_list, train_y_list, test_x_list = load_train_test(\"train_x.txt\",\"train_y.txt\",\"test_x.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "class Vocab():\n",
    "    def __init__(self, word_count=None):\n",
    "        self.id2word = {0:\"<UNK>\", 1:\"<START>\" , 2:\"<END>\", 3:\"<PAD>\"}\n",
    "        self.word2id = {\"<UNK>\":0, \"<START>\":1, \"<END>\":2, \"<PAD>\":3}\n",
    "#         self.word_count = {\"UNK\":0}\n",
    "        self.size = 5\n",
    "        \n",
    "    def add(self, word):\n",
    "#         if word in self.word2id:\n",
    "#             self.word_count[word] += 1\n",
    "#         else:\n",
    "        self.id2word[self.size] = word\n",
    "        self.word2id[word] = self.size\n",
    "#             self.word_count[word] = 1\n",
    "        self.size += 1  \n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def save(self, path=\"vocab.txt\"):\n",
    "        with open(path , \"w\") as f:\n",
    "            for word, idx in self.word2id.items():\n",
    "                f.write(f\"{word} {idx}\\n\")\n",
    "        print(f\"vocab saved in {path}, vocab size: {self.size}\")\n",
    "                \n",
    "    def reload(self, path=\"vocab.txt\"):\n",
    "        with open(path, \"r\") as f:\n",
    "            idx = 1\n",
    "            for line in f.readlines():\n",
    "                sentence = line.strip().split()\n",
    "                self.id2word[idx] = word\n",
    "                self.word2id[word] = idx\n",
    "        self.size =len(self.id2word)\n",
    "        print(f\"vocab loaded from {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab saved in vocab.txt, vocab size: 22063\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_vocab(threshold=0.9):\n",
    "    \"统计词频，根据给定的阈值筛选后，保存至词典\"\n",
    "    all_context_list = []\n",
    "    for x in [train_x_list, train_y_list, test_x_list, val_x_list, val_y_list]:\n",
    "        for y in x:\n",
    "            for word in y:\n",
    "                all_context_list.append(word)\n",
    "                \n",
    "    cnt = Counter(all_context_list)\n",
    "    all_words_num = len(cnt)\n",
    "    words_num_after = int(threshold * all_words_num)\n",
    "    print(f\"all words num {all_words_num}, words after {words_num_after}, {all_words_num - words_num_after} were throwed\")\n",
    "    \n",
    "    vocab = Vocab()\n",
    "    for char, freq in cnt.most_common(words_num_after):\n",
    "        if freq > 3:\n",
    "            vocab.add(char)\n",
    "    print(f\"vocab size {len(vocab)}\")\n",
    "    vocab.save()\n",
    "    return vocab\n",
    "       \n",
    "vocab = build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.input_size = 200\n",
    "        self.hidden_size = 64\n",
    "        self.dropout = 0.5\n",
    "        self.num_layers = 1\n",
    "        self.bidirectional = True\n",
    "        self.batch_size = 1\n",
    "        self.input_max_len = 256\n",
    "        self.output_max_len = 64\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, config, vocab, mode=\"train\", num_data=None):\n",
    "        self.mode = mode\n",
    "        self.input_max_len = config.input_max_len\n",
    "        self.output_max_len = config.output_max_len\n",
    "        self.vocab = vocab\n",
    "        if mode==\"train\":\n",
    "            self.train_x = []\n",
    "            self.train_y = []\n",
    "#             self.size = len(self.train_x)\n",
    "            \n",
    "            with open(\"train_x.txt\", \"r\") as f:\n",
    "                for line in tqdm(f.readlines()[:num_data]):\n",
    "                    line = [s for s in line.strip().split(\" \") if s != \"\"]\n",
    "                    \n",
    "                    if len(line) < self.input_max_len:\n",
    "                        line += [\"<PAD>\"]*(self.input_max_len - len(line))\n",
    "                    else:\n",
    "                        line = line[:self.input_max_len]\n",
    "                        \n",
    "                    self.train_x.append(line)\n",
    "                    \n",
    "            with open(\"train_y.txt\", \"r\") as f:\n",
    "                for line in tqdm(f.readlines()[:num_data]):\n",
    "                    line = [s for s in line.strip().split(\" \") if s != \"\"]\n",
    "                    if len(line) < self.output_max_len:\n",
    "                        line += [\"<END>\"] + [\"<PAD>\"] * (self.output_max_len - len(line)-1)  \n",
    "                    else:\n",
    "                        line = line[:self.output_max_len-1] + [\"<END>\"]\n",
    "                        \n",
    "                    self.train_y.append(line)\n",
    "                    \n",
    "            self.size = len(self.train_x)\n",
    "        elif mode==\"eval\":\n",
    "            self.eval_x = []\n",
    "            self.eval_y = []\n",
    "#             self.size = len(self.train_x)\n",
    "            with open(\"train_x.txt\", \"r\") as f: \n",
    "                for line in tqdm(f.readlines()[-num_data:]):\n",
    "                    line = [s for s in line.strip().split(\" \") if s != \"\"]\n",
    "                    if len(line) < self.input_max_len:\n",
    "                        line += [\"<PAD>\"]*(self.input_max_len - len(line))\n",
    "                    else:\n",
    "                        line = line[:self.input_max_len]\n",
    "                    self.eval_x.append(line)\n",
    "                    \n",
    "            with open(\"train_y.txt\", \"r\") as f:\n",
    "                for line in tqdm(f.readlines()[-num_data:]):\n",
    "                    line = [s for s in line.strip().split(\" \") if s != \"\"]\n",
    "                    self.eval_y.append(line)\n",
    "            self.size = len(self.eval_x)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            self.test_x = []\n",
    "            with open(\"test_x.txt\", \"r\") as f:\n",
    "                for line in tqdm(f.readlines()[:num_data]):\n",
    "                    line = [s for s in line.strip().split(\" \") if s != \"\"]\n",
    "                    if len(line) < self.input_max_len:\n",
    "                        line += [\"<PAD>\"]*(self.input_max_len - len(line))\n",
    "                    else:\n",
    "                        line = line[:self.input_max_len]\n",
    "                    self.test_x.append(line)\n",
    "            self.size = len(self.test_x)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode==\"train\":\n",
    "            train_x_tokens = []\n",
    "            for x in self.train_x[idx]:\n",
    "                if x not in self.vocab.word2id:\n",
    "                    train_x_tokens.append(self.vocab.word2id[\"<UNK>\"])\n",
    "                else:\n",
    "                    train_x_tokens.append(self.vocab.word2id[x])\n",
    "            \n",
    "            train_y_tokens = []\n",
    "            for x in self.train_y[idx]:\n",
    "                if x not in self.vocab.word2id:\n",
    "                    train_y_tokens.append(self.vocab.word2id[\"<UNK>\"])\n",
    "                else:\n",
    "                    train_y_tokens.append(self.vocab.word2id[x])\n",
    "            \n",
    "            return torch.tensor(train_x_tokens), torch.tensor(train_y_tokens)\n",
    "        \n",
    "        elif self.mode == \"eval\":\n",
    "            eval_x_tokens = []\n",
    "            for x in self.eval_x[idx]:\n",
    "                if x not in self.vocab.word2id:\n",
    "                    eval_x_tokens.append(self.vocab.word2id[\"<UNK>\"])\n",
    "                else:\n",
    "                    eval_x_tokens.append(self.vocab.word2id[x])\n",
    "            \n",
    "            return torch.tensor(eval_x_tokens), self.eval_y[idx]\n",
    "        \n",
    "        else:\n",
    "            test_x_tokens = []\n",
    "            for x in self.test_x[idx]:\n",
    "                if x not in self.vocab.word2id:\n",
    "                    test_x_tokens.append(self.vocab.word2id[\"<UNK>\"])\n",
    "                else:\n",
    "                    test_x_tokens.append(self.vocab.word2id[x])\n",
    "            return torch.tensor(test_x_tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1044,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:01<00:00, 16006.00it/s]\n",
      "100%|██████████| 30000/30000 [00:00<00:00, 163094.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 19971.93it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 59451.51it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 50445.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TextDataset(config,vocab, \"train\",1000)\n",
    "print(len(train_dataset))\n",
    "eval_dataset = TextDataset(config,vocab, \"eval\",100)\n",
    "print(len(eval_dataset))\n",
    "test_dataset = TextDataset(config,vocab, \"test\", 500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\".join([vocab.id2word[x.item()] for x in sample[0][0]]))\n",
    "# print(sample[0][1])\n",
    "# # print(\"\".join([vocab.id2word[x.item()] for x in sample[1]]))\n",
    "# print(\"\".join([vocab.id2word[x.item()] for x in sample_t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1046,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True) \n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 50]) torch.Size([64, 20]) torch.Size([1, 50])\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = next(iter(train_dataloader)) # seq_len, batch_size * 1\n",
    "test_x = next(iter(test_dataloader)) # seq_len, batch_size * 1\n",
    "val_x, val_y = next(iter(eval_dataloader))\n",
    "print(train_x.shape, train_y.shape, test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, config, vocab):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.input_size = config.input_size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_layers = config.num_layers\n",
    "        self.bidirectional = config.bidirectional\n",
    "        self.batch_size = config.batch_size\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(len(self.vocab), self.input_size)\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(self.input_size, self.hidden_size, num_layers=self.num_layers\n",
    "                                  , batch_first=True, bidirectional=self.bidirectional)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, inputs, hidden=None, cell=None):\n",
    "        '''\n",
    "        inputs: batch_size, seq_len\n",
    "        init_hidden/init_cell: numb_layers * 2, batch_size, hidden_size\n",
    "        ''' \n",
    "#         print(\"encoder input (batch_size, seq_len): \", inputs.shape) \n",
    "        inputs = self.embedding(inputs)\n",
    "#         print(\"encoder input after embed (batch_size, seq_len, input_size):\", inputs.shape)\n",
    "        # inputs after embedding: batch_size, seq_len, input_size\n",
    "        if hidden is not None:\n",
    "            outputs, (hidden, cell) = self.lstm(inputs, (hidden, cell))\n",
    "#             print(\"final output shape: \", outputs.shape)\n",
    "#             print(\"final hidden shape: \", hidden.shape)\n",
    "#             print(\"final cell shape: \", cell.shape)\n",
    "        else:\n",
    "            outputs, (hidden, cell) = self.lstm(inputs)\n",
    "#             print(\"final output shape: \", outputs.shape)\n",
    "#             print(\"final hidden shape: \", hidden.shape)\n",
    "#             print(\"final cell shape: \", cell.shape)\n",
    "        \n",
    "\n",
    "        return outputs, hidden, cell\n",
    "    \n",
    "    def init_hidden_cell(self):\n",
    "        D = self.num_layers * 2 if self.bidirectional else self.num_layers\n",
    "        return torch.zeros(D, self.batch_size, self.hidden_size),torch.zeros(D, self.batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(config,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1050,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 50, 24]), torch.Size([2, 64, 12]), torch.Size([2, 64, 12]))"
      ]
     },
     "execution_count": 1050,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_hidden, init_cell = encoder.init_hidden_cell()\n",
    "out, hidden, cell = encoder(train_x, init_hidden, init_cell)\n",
    "out.shape, hidden.shape, cell.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, config, vocab):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.input_size = config.input_size\n",
    "        self.output_size = len(vocab)\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.dropout = torch.nn.Dropout(config.dropout)\n",
    "        self.num_layers = config.num_layers\n",
    "        self.bidirectional = config.bidirectional\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(len(self.vocab), self.input_size)\n",
    "        self.lstm = torch.nn.LSTM(self.input_size, self.hidden_size, num_layers=self.num_layers\n",
    "                                  , batch_first=True, bidirectional=self.bidirectional)\n",
    "        self.fc = torch.nn.Linear(self.hidden_size * 2, self.output_size)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, inputs, hidden, cell):\n",
    "        '''\n",
    "        inputs: batch_size, seq_len\n",
    "        init_hidden/init_cell: numb_layers * 2, batch_size, input_size\n",
    "        '''\n",
    "#         print(\"decoder input:\", inputs.shape)\n",
    "        inputs = self.embedding(inputs) \n",
    "#         print(\"decoder input after embed:\", inputs.shape)\n",
    "#         inputs after embedding: batch_size, seq_len, input_size\n",
    "#         print(\"decoder input after embed and permute (batch_size,seq_len,input_size):\", inputs.shape)\n",
    "#         print(\"hidden: \", hidden.shape)\n",
    "        outputs, (hidden, cell) = self.lstm(inputs, (hidden, cell))\n",
    "#         print(\"outputs shape: \", outputs.shape)\n",
    "        prediction = self.softmax(self.fc(torch.squeeze(outputs, 1)))\n",
    "#         print(f\"prediction: {prediction.shape}    outputs: {outputs.shape}\")\n",
    "        return prediction, outputs, hidden, cell\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(config, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 1053,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input = torch.tensor([vocab.word2id[\"<START>\"]] * config.batch_size).view(-1,1)\n",
    "# decoder_input = torch.unsqueeze(decoder_input, 1)\n",
    "decoder_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions---- torch.Size([64, 22063])\n",
      "outputs---- torch.Size([64, 1, 24])\n",
      "pred_token: tensor([ 9331, 20323, 21070, 21070, 15452,  1996, 21414, 21070, 20442, 21414,\n",
      "        20323, 20442, 20442, 21070,  1996, 20106, 20106, 20106,  5269, 21070,\n",
      "        16666,  9811,  1869,  2731, 21070, 20442,  7465, 21005, 21070, 21070,\n",
      "         9811, 21070, 21070,  9160, 11217,  1996, 21244, 21070,  4879, 21070,\n",
      "        16883,  1264, 21070, 21070,  3124, 20442, 21070, 20442, 21711, 10742,\n",
      "        21070, 21070, 20323,  9811,  1996, 21070, 21070, 20106, 21070,  8152,\n",
      "        21414,  1996, 21414, 20913])\n",
      "next hidden: torch.Size([2, 64, 12])\n"
     ]
    }
   ],
   "source": [
    "prediction, outputs, next_hidden, next_cell = decoder(decoder_input, hidden, cell)\n",
    "print(\"predictions----\", prediction.shape)\n",
    "print(\"outputs----\", outputs.shape)\n",
    "# print(\"next_hidden----\", next_hidden.shape)\n",
    "predict_token = torch.argmax(prediction,1)\n",
    "# predict_token, vocab.id2word[predict_token.item()]\n",
    "\n",
    "print(\"pred_token:\", predict_token)\n",
    "print(f\"next hidden: {next_hidden.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 1055,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decoder_input = torch.tensor([predict_token])\n",
    "decoder_input = predict_token.view(-1,1)\n",
    "decoder_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 1056,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prediction, final_outputs, final_hidden, final_cell = decoder(decoder_input, next_hidden, next_cell)\n",
    "final_predict_token = torch.argmax(final_prediction,1)\n",
    "final_predict_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(torch.nn.Module):\n",
    "    def __init__(self, config, vocab):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.config = config\n",
    "        self.input_max_len = config.input_max_len\n",
    "        self.output_max_len = config.output_max_len\n",
    "        self.encoder = Encoder(config,vocab)\n",
    "        self.decoder = Decoder(config,vocab)\n",
    "        \n",
    "    def forward(self, encoder_input, target=None):\n",
    "        target_len = self.output_max_len\n",
    "        batch_size = encoder_input.shape[0]\n",
    "#         if target is not None:\n",
    "#             batch_size = self.config.batch_size\n",
    "#         else:\n",
    "#             batch_size = 1\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, target_len, len(vocab))\n",
    "        \n",
    "        \n",
    "        decoder_input = torch.tensor([vocab.word2id[\"<START>\"]] * batch_size).view(-1,1)\n",
    "        \n",
    "        encoder_outputs, hiddens, cells = encoder(encoder_input)\n",
    "#         predictions = []\n",
    "#         print(\"-------encoder-------\")\n",
    "#         print(\"encoder outputs: \", encoder_outputs.shape)\n",
    "#         print(\"encoder hiddens: \", hiddens.shape)\n",
    "#         print(\"encoder cells: \", cells.shape)\n",
    "#         print(\"-------decoder-------\")\n",
    "        for i in range(target_len):\n",
    "#             print(f\"---------step {i}----------\")\n",
    "#             print(\"decoder outputs: \", encoder_outputs.shape)\n",
    "#             print(\"decoder hiddens: \", hiddens.shape)\n",
    "#             print(\"decoder cells: \", cells.shape)\n",
    "            next_predictions, next_outputs, hiddens, cells = decoder(decoder_input, hiddens, cells)\n",
    "            outputs[:,i,:] = next_predictions\n",
    "#             predictions[:,i] = \n",
    "#             predictions_tokens = next_predictions.argmax(1)\n",
    "#             predictions.append(predictions_tokens)\n",
    "            \n",
    "#             print([vocab.id2word[x] for x in predictions_tokens])\n",
    "        return outputs     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(config,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1059,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outputs = model(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1060,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 20, 22063])"
      ]
     },
     "execution_count": 1060,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1061,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 20, 22063]), torch.Size([64, 20]))"
      ]
     },
     "execution_count": 1061,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1062,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 20])"
      ]
     },
     "execution_count": 1062,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_PATH  = 'best.pkl'\n",
    "predictions = outputs.argmax(2)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-39e91ebde41c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbatch_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": [
    "def loss_fn(outputs, train_y):\n",
    "    '''\n",
    "    outputs: batch_size, seq_len, input_size\n",
    "    train_y: batch_size, seq_len\n",
    "    '''\n",
    "    loss_obj = torch.nn.NLLLoss(reduction='mean')\n",
    "    batch_loss = 0\n",
    "    batch_size = train_y.shape[1]\n",
    "    for i in range(batch_size):\n",
    "        batch_outputs = outputs[:,i,:]\n",
    "        batch_y = train_y[:,i]\n",
    "        batch_loss += loss_obj(batch_outputs, batch_y)\n",
    "    return batch_loss / batch_size\n",
    "\n",
    "loss_fn(outputs,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1064,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 1064,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lcs(s1, s2):\n",
    "    matrix = [ [0 for x in range(len(s2))] for x in range(len(s1)) ]\n",
    "#     cs = \"\"\n",
    "    for i in range(len(s1)):\n",
    "        for j in range(len(s2)):\n",
    "            if s1[i]==s2[j]:\n",
    "                if i==0 or j==0:\n",
    "                    matrix[i][j] = 1\n",
    "#                     cs += s1[i]\n",
    "                else:\n",
    "                    matrix[i][j] = matrix[i-1][j-1] + 1\n",
    "#                     cs += s1[i]\n",
    "            else:\n",
    "                if i==0 or j==0:\n",
    "                    matrix[i][j] = 0\n",
    "                else:\n",
    "                    matrix[i][j] = max(matrix[i-1][j], matrix[i][j-1])\n",
    "\n",
    "    return matrix[len(s1)-1][len(s2)-1]\n",
    "\n",
    "lcs([\"a\", \"b\",\"c\", \"e\", \"f\"], [\"c\", \"e\", \"f\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1065,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 1065,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rouge(pred, target):\n",
    "    beta = 100\n",
    "    temp_lcs = lcs(pred, target)\n",
    "    r_lcs = temp_lcs / len(target)\n",
    "    p_lcs = temp_lcs / len(pred)\n",
    "    up = (1 + beta**2) * r_lcs * p_lcs\n",
    "    down = r_lcs + beta**2 * p_lcs + 0.001\n",
    "    return up/down\n",
    "\n",
    "def evaluate(model, eval_dataloader):\n",
    "#     print(eval_dataset[0][1])\n",
    "    model.eval()\n",
    "    total_rouge = 0\n",
    "    with torch.no_grad():\n",
    "        for dev_x, target in eval_dataloader:\n",
    "            target = [vocab.id2word[x.item()] for x in target[0]]\n",
    "            outputs = model(dev_x)\n",
    "            predictions = outputs.argmax(2)[0] # predictions: 64,50\n",
    "#             print(f\"----------predictions {predictions.shape}--------------\")\n",
    "            result = []\n",
    "            for one_pred in predictions:\n",
    "#                 one_line_tokens = one_pred\n",
    "                pred = vocab.id2word[one_pred.item()]\n",
    "                if pred == \"<END>\": break\n",
    "                result.append(pred)\n",
    "            \n",
    "            total_rouge += rouge(result, target)\n",
    "    return total_rouge\n",
    "         \n",
    "evaluate(model, eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1066,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, eval_dataloader, epoch_num):\n",
    "#     total_loss = 0\n",
    "    loss_ = loss_fn\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "    for epoch in range(epoch_num):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        best_test_loss = 1000\n",
    "        print(f\"-----------------epoch {epoch}-------------------\")\n",
    "        for step, (train_x, train_y) in enumerate(train_dataloader):\n",
    "            outputs = model(train_x, train_y)\n",
    "#             print(outputs.shape, train_y.shape)\n",
    "            loss = loss_(outputs, train_y)\n",
    "            # 单向传播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss\n",
    "            if step // 10 == 0: \n",
    "                print(f\"step {step}  loss {loss.item()} average_loss {total_loss / (step + 1)}\")\n",
    "                predictions = outputs.argmax(2)[0]\n",
    "                result = []\n",
    "                target_sentence = []\n",
    "                for tr in train_y[0]:\n",
    "                    tr = vocab.id2word[tr.item()]\n",
    "                    target_sentence.append(tr)\n",
    "                for one_pred in predictions:\n",
    "    #                 one_line_tokens = one_pred\n",
    "                    pred = vocab.id2word[one_pred.item()]\n",
    "                    if pred == \"<END>\": break\n",
    "                    result.append(pred)\n",
    "                \n",
    "                print(f\"sample result: {result[:10]} \\ntarget sentence: {target_sentence}\")\n",
    "        print(f\"batch ROUGE-L: {evaluate(model, eval_dataloader)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1067,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------epoch 0-------------------\n",
      "step 0  loss 9.964241981506348 average_loss 9.964241981506348\n",
      "sample result: ['手指', '型号', '型号', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 1  loss 9.995153427124023 average_loss 9.979698181152344\n",
      "sample result: ['故障诊断', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 2  loss 9.979910850524902 average_loss 9.979769706726074\n",
      "sample result: ['886', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 3  loss 9.991281509399414 average_loss 9.982646942138672\n",
      "sample result: ['型号', '型号', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 4  loss 9.998929023742676 average_loss 9.985902786254883\n",
      "sample result: ['3s', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 5  loss 10.00325870513916 average_loss 9.988795280456543\n",
      "sample result: ['手指', '886', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 6  loss 9.98540210723877 average_loss 9.988310813903809\n",
      "sample result: ['丝扣', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 7  loss 9.992122650146484 average_loss 9.988786697387695\n",
      "sample result: ['886', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 8  loss 10.020963668823242 average_loss 9.992362022399902\n",
      "sample result: ['886', '回话', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 9  loss 9.980440139770508 average_loss 9.991169929504395\n",
      "sample result: ['倒角', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "batch ROUGE-L: 0.0\n",
      "-----------------epoch 1-------------------\n",
      "step 0  loss 9.987395286560059 average_loss 9.987395286560059\n",
      "sample result: ['接常', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 1  loss 10.014409065246582 average_loss 10.00090217590332\n",
      "sample result: ['手指', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 2  loss 9.990720748901367 average_loss 9.99750804901123\n",
      "sample result: ['我心', '型号', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 3  loss 9.996978759765625 average_loss 9.99737548828125\n",
      "sample result: ['手指', '型号', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 4  loss 9.998198509216309 average_loss 9.997540473937988\n",
      "sample result: ['3s', '空转', '回话', '回话', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 5  loss 10.010313034057617 average_loss 9.999669075012207\n",
      "sample result: ['刚开', '型号', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 6  loss 9.999818801879883 average_loss 9.999690055847168\n",
      "sample result: ['回话', '回话', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 7  loss 9.993534088134766 average_loss 9.998920440673828\n",
      "sample result: ['GL350', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 8  loss 9.990639686584473 average_loss 9.998000144958496\n",
      "sample result: ['向导', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 9  loss 9.978200912475586 average_loss 9.996020317077637\n",
      "sample result: ['型号', '型号', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "batch ROUGE-L: 0.0\n",
      "-----------------epoch 2-------------------\n",
      "step 0  loss 9.994832992553711 average_loss 9.994832992553711\n",
      "sample result: ['六角', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 1  loss 9.980243682861328 average_loss 9.98753833770752\n",
      "sample result: ['向导', '回话', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 2  loss 10.017207145690918 average_loss 9.997427940368652\n",
      "sample result: ['纯净', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 3  loss 9.96918773651123 average_loss 9.990367889404297\n",
      "sample result: ['型号', '型号', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 4  loss 9.965662956237793 average_loss 9.985426902770996\n",
      "sample result: ['夹', '886', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 5  loss 9.999503135681152 average_loss 9.987772941589355\n",
      "sample result: ['纯净', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 6  loss 9.980746269226074 average_loss 9.986769676208496\n",
      "sample result: ['886', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 7  loss 10.004551887512207 average_loss 9.988992691040039\n",
      "sample result: ['纯净', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 8  loss 10.019156455993652 average_loss 9.99234390258789\n",
      "sample result: ['886', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 9  loss 9.98019790649414 average_loss 9.991129875183105\n",
      "sample result: ['悬置', '型号', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "batch ROUGE-L: 0.0\n",
      "-----------------epoch 3-------------------\n",
      "step 0  loss 9.999521255493164 average_loss 9.999521255493164\n",
      "sample result: ['886', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 1  loss 9.983480453491211 average_loss 9.991500854492188\n",
      "sample result: ['886', '型号', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 2  loss 9.974599838256836 average_loss 9.985867500305176\n",
      "sample result: ['886', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 3  loss 9.971296310424805 average_loss 9.982224464416504\n",
      "sample result: ['3s', '汇报', '回话', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 4  loss 9.98339557647705 average_loss 9.982458114624023\n",
      "sample result: ['886', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 5  loss 9.976412773132324 average_loss 9.981451034545898\n",
      "sample result: ['型号', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 6  loss 10.004114151000977 average_loss 9.984688758850098\n",
      "sample result: ['型号', '型号', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 7  loss 9.984027862548828 average_loss 9.98460578918457\n",
      "sample result: ['汇报', '汇报', '回话', '回话', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 8  loss 9.973179817199707 average_loss 9.983336448669434\n",
      "sample result: ['型号', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 9  loss 9.98542308807373 average_loss 9.983545303344727\n",
      "sample result: ['886', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "batch ROUGE-L: 0.0\n",
      "-----------------epoch 4-------------------\n",
      "step 0  loss 9.966829299926758 average_loss 9.966829299926758\n",
      "sample result: ['型号', '型号', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 1  loss 9.992439270019531 average_loss 9.979634284973145\n",
      "sample result: ['886', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 2  loss 10.000227928161621 average_loss 9.986498832702637\n",
      "sample result: ['向导', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 3  loss 10.008421897888184 average_loss 9.991979598999023\n",
      "sample result: ['连电', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 4  loss 10.02612590789795 average_loss 9.998808860778809\n",
      "sample result: ['向导', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 5  loss 9.985673904418945 average_loss 9.996620178222656\n",
      "sample result: ['回话', '回话', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 6  loss 10.002344131469727 average_loss 9.997437477111816\n",
      "sample result: ['汇报', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 7  loss 9.996109008789062 average_loss 9.997271537780762\n",
      "sample result: ['为啥', '档锁止', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 8  loss 9.992631912231445 average_loss 9.996755599975586\n",
      "sample result: ['左下方', '档锁止', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "step 9  loss 9.975776672363281 average_loss 9.994657516479492\n",
      "sample result: ['886', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌', '锁舌']\n",
      "batch ROUGE-L: 0.0\n"
     ]
    }
   ],
   "source": [
    "train(model, train_dataloader, eval_dataloader, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
